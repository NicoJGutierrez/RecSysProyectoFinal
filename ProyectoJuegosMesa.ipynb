{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jH8fyVEjke2x"
   },
   "source": [
    "# Propuesta Proyecto RecSys: Recomendación Grupal de Juegos de Mesa (setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygicq0KDTnqq"
   },
   "source": [
    "Link de dataset: https://www.kaggle.com/datasets/threnjen/board-games-database-from-boardgamegeek?select=games.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMyiOuV3TyEZ",
    "outputId": "c7b45953-faab-45cb-c214-110509412021"
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!pip install pyreclab\n",
    "!pip install implicit\n",
    "!pip install surprise\n",
    "!pip install elliot\n",
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzALElwIUwwS"
   },
   "source": [
    "Tutorial usado: https://www.kaggle.com/discussions/general/74235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu7OpwbIW4Hu"
   },
   "source": [
    "Tutorial adicional usado: https://www.youtube.com/watch?v=yEXkEUqK52Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8V2_lleq2w5",
    "outputId": "bc5cc37a-a6fd-454f-8565-5331a610e4ad"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"threnjen/board-games-database-from-boardgamegeek\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xKgz0X2q2w6"
   },
   "source": [
    "Arriba de esto aparecerá un \"path to dataset files\", ese path se debe copiar y pegar en la línea de abajo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Y1R8PyNW2pp"
   },
   "outputs": [],
   "source": [
    "path_to_dataset_files = path\n",
    "#path_to_dataset_files = '/home/nico/.cache/kagglehub/datasets/threnjen/board-games-database-from-boardgamegeek/versions/4'\n",
    "#path_to_dataset_files = '/root/.cache/kagglehub/datasets/threnjen/board-games-database-from-boardgamegeek/versions/4'\n",
    "\n",
    "import os\n",
    "# Guardamos el directorio actual\n",
    "base_dir = os.getcwd()\n",
    "# Cambiamos al directorio donde se encuentra el dataset\n",
    "os.chdir(path_to_dataset_files)\n",
    "\n",
    "# Importamos las librerias\n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreclab\n",
    "import tempfile\n",
    "import implicit\n",
    "import random\n",
    "from surprise import accuracy\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZBjZKWDK2Uq"
   },
   "source": [
    "Generamos los datos a utilizar como un muestreo del dataset original pues es muy grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX_VaAH9Xs-T",
    "outputId": "c7505886-1461-4785-b812-d0a970744e33"
   },
   "outputs": [],
   "source": [
    "# Leemos el csv y volvemos al directorio base del proyecto\n",
    "user_ratings = pd.read_csv('user_ratings.csv')\n",
    "mechanics_df = pd.read_csv('mechanics.csv')\n",
    "# Volvemos al directorio base del proyecto\n",
    "os.chdir(base_dir)\n",
    "print(base_dir)\n",
    "\n",
    "# Cambiamos username por un userid\n",
    "a=list(set(list(user_ratings.Username)))\n",
    "d = {a[i]: i for i in range(len(a))}\n",
    "a_mod = [d[i] for i in list(user_ratings.Username)]\n",
    "user_ratings[\"Username\"] = a_mod\n",
    "\n",
    "# Separamos training y testing\n",
    "train     = list(set(user_ratings.Username))[:8000]\n",
    "test      = list(set(user_ratings.Username))[8000:11000]\n",
    "test_set  = user_ratings[user_ratings[\"Username\"].isin(test)].sample(9000)\n",
    "train_set = user_ratings[user_ratings[\"Username\"].isin(train)].sample(35000)\n",
    "\n",
    "# Generamos nuevo csv de training y testing\n",
    "train_set.to_csv(\"train.csv\", index=False, sep=',', header=True)\n",
    "test_set.to_csv(\"test.csv\", index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vcXoaD-wYcB"
   },
   "source": [
    "# Creación de grupos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código importado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pni1vfFjwcCX"
   },
   "outputs": [],
   "source": [
    "# Groups generator from: https://github.com/barnap/group-recommenders-offline-evaluation/blob/main/synthetic_groups_generation/groups_generators.py\n",
    "\n",
    "class GroupsGenerator(ABC):\n",
    "\n",
    "    @staticmethod\n",
    "    def getGroupsGenerator(type):\n",
    "        if type == \"RANDOM\":\n",
    "            return RandomGroupsGenerator()\n",
    "        elif type == \"SIMILAR\":\n",
    "            return SimilarGroupsGenerator()\n",
    "        elif type == \"DIVERGENT\":\n",
    "            return DivergentGroupsGenerator()\n",
    "        elif type == \"SIMILAR_ONE_DIVERGENT\":\n",
    "            return MinorityGroupsGenerator()\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_average_similarity(group, user_id_indexes, sim_matrix):\n",
    "        similarities = list()\n",
    "        for user_1 in group:\n",
    "            user_1_index = user_id_indexes.tolist().index(user_1)\n",
    "            for user_2 in group:\n",
    "                user_2_index = user_id_indexes.tolist().index(user_2)\n",
    "                if user_1 != user_2:\n",
    "                    similarities.append(sim_matrix[user_1_index][user_2_index])\n",
    "        return np.mean(similarities)\n",
    "\n",
    "    @abstractmethod\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomGroupsGenerator(GroupsGenerator):\n",
    "\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        groups_list = list()\n",
    "        for group_size in group_sizes_to_create:\n",
    "            for i in range(group_number_to_create):\n",
    "                group = random.sample(user_id_set, group_size)\n",
    "                groups_list.append(\n",
    "                    {\n",
    "                        \"group_size\": group_size,\n",
    "                        \"group_similarity\": 'random',\n",
    "                        \"group_members\": group,\n",
    "                        \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n",
    "                    }\n",
    "                )\n",
    "            print(len(groups_list))\n",
    "        return groups_list\n",
    "\n",
    "\n",
    "class SimilarGroupsGenerator(GroupsGenerator):\n",
    "\n",
    "    @staticmethod\n",
    "    def select_user_for_sim_group(group, sim_matrix, user_id_indexes, sim_threshold=0.4):\n",
    "        '''\n",
    "        Helper function to the generate_similar_user_groups function. Given already selected group members, it randomly\n",
    "        selects from the remaining users that has a PCC value >= sim_threshold to any of the existing members.\n",
    "        :param group:\n",
    "        :param sim_matrix:\n",
    "        :param user_id_indexes:\n",
    "        :param sim_threshold:\n",
    "        :return:\n",
    "        '''\n",
    "        ids_to_select_from = set()\n",
    "        for member in group:\n",
    "            member_index = user_id_indexes.tolist().index(member)\n",
    "            indexes = np.where(sim_matrix[member_index] >= sim_threshold)[0].tolist()\n",
    "            user_ids = [user_id_indexes[index] for index in indexes]\n",
    "            ids_to_select_from = ids_to_select_from.union(set(user_ids))\n",
    "        candidate_ids = ids_to_select_from.difference(set(group))\n",
    "        if len(candidate_ids) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            selection = random.sample(candidate_ids, 1)\n",
    "            return selection[0]\n",
    "\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        groups_list = list()\n",
    "        for group_size in group_sizes_to_create:\n",
    "            groups_size_list = list()\n",
    "            while (len(groups_size_list) < group_number_to_create):\n",
    "                group = random.sample(user_id_set, 1)\n",
    "                while len(group) < group_size:\n",
    "                    new_member = SimilarGroupsGenerator.select_user_for_sim_group(group, similarity_matrix,\n",
    "                                                                                  user_id_indexes,\n",
    "                                                                                  sim_threshold=0.5)\n",
    "                    if new_member is None:\n",
    "                        break\n",
    "                    group.append(new_member)\n",
    "                if len(group) == group_size:\n",
    "                    groups_size_list.append(\n",
    "                        {\n",
    "                            \"group_size\": group_size,\n",
    "                            \"group_similarity\": 'similar',\n",
    "                            \"group_members\": group,\n",
    "                            \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n",
    "                        }\n",
    "                    )\n",
    "            groups_list.extend(groups_size_list)\n",
    "            print(len(groups_list))\n",
    "        return groups_list\n",
    "\n",
    "\n",
    "class DivergentGroupsGenerator(GroupsGenerator):\n",
    "\n",
    "    @staticmethod\n",
    "    def select_user_for_divergent_group(group, sim_matrix, user_id_indexes, sim_threshold=0.0):\n",
    "        '''\n",
    "        Helper function to the generate_similar_user_groups function. Given already selected group members, it randomly\n",
    "        selects from the remaining users that has a PCC value < sim_threshold to any of the existing members.\n",
    "        :param group:\n",
    "        :param sim_matrix:\n",
    "        :param user_id_indexes:\n",
    "        :param sim_threshold:\n",
    "        :return:\n",
    "        '''\n",
    "        ids_to_select_from = set()\n",
    "        for member in group:\n",
    "            member_index = user_id_indexes.tolist().index(member)\n",
    "            indexes = np.where(sim_matrix[member_index] < sim_threshold)[0].tolist()\n",
    "            user_ids = [user_id_indexes[index] for index in indexes]\n",
    "            ids_to_select_from = ids_to_select_from.union(set(user_ids))\n",
    "        candidate_ids = ids_to_select_from.difference(set(group))\n",
    "        if len(candidate_ids) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            selection = random.sample(candidate_ids, 1)\n",
    "            return selection[0]\n",
    "\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        groups_list = list()\n",
    "        for group_size in group_sizes_to_create:\n",
    "            groups_size_list = list()\n",
    "            while (len(groups_size_list) < group_number_to_create):\n",
    "                group = random.sample(user_id_set, 1)\n",
    "                while len(group) < group_size:\n",
    "                    new_member = DivergentGroupsGenerator.select_user_for_divergent_group(group, similarity_matrix,\n",
    "                                                                                     user_id_indexes,\n",
    "                                                                                     sim_threshold=-0.1)\n",
    "                    if new_member is None:\n",
    "                        break\n",
    "                    group.append(new_member)\n",
    "                if len(group) == group_size:\n",
    "                    groups_size_list.append(\n",
    "                        {\n",
    "                            \"group_size\": group_size,\n",
    "                            \"group_similarity\": 'divergent',\n",
    "                            \"group_members\": group,\n",
    "                            \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n",
    "                        }\n",
    "                    )\n",
    "            groups_list.extend(groups_size_list)\n",
    "            print(len(groups_list))\n",
    "        return groups_list\n",
    "\n",
    "\n",
    "class MinorityGroupsGenerator(GroupsGenerator):\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        groups_list = list()\n",
    "        for group_size in group_sizes_to_create:\n",
    "            groups_size_list = list()\n",
    "            while (len(groups_size_list) < group_number_to_create):\n",
    "                group = random.sample(user_id_set, 1)\n",
    "                while len(group) < (group_size - 1):\n",
    "                    new_member = SimilarGroupsGenerator.select_user_for_sim_group(group, similarity_matrix,\n",
    "                                                                                     user_id_indexes,\n",
    "                                                                                     sim_threshold=0.5)\n",
    "                    if new_member is None:\n",
    "                        break\n",
    "                    group.append(new_member)\n",
    "\n",
    "                dissimilar_member = DivergentGroupsGenerator.select_user_for_divergent_group(group, similarity_matrix,\n",
    "                                                                                              user_id_indexes,\n",
    "                                                                                              sim_threshold=-0.1)\n",
    "                if dissimilar_member is not None:\n",
    "                    group.append(dissimilar_member)\n",
    "                if len(group) == group_size:\n",
    "                    groups_size_list.append(\n",
    "                        {\n",
    "                            \"group_size\": group_size,\n",
    "                            \"group_similarity\": 'similar_one_divergent',\n",
    "                            \"group_members\": group,\n",
    "                            \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n",
    "                        }\n",
    "                    )\n",
    "            groups_list.extend(groups_size_list)\n",
    "            print(len(groups_list))\n",
    "        return groups_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código nuestro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh7Zuy7SN1QJ"
   },
   "source": [
    "Informacion para crear grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56xPcl69N0i6"
   },
   "outputs": [],
   "source": [
    "group_sizes_to_create = [4]        # [2, 3, 4, 5, 6, 7, 8]\n",
    "group_similarity_to_create = \"RANDOM\"  # [\"RANDOM\", \"SIMILAR\", \"DIVERGENT\", \"SIMILAR_ONE_DIVERGENT\"]\n",
    "group_number = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck2euynXwvOp"
   },
   "source": [
    "Creacion de los grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlVeyb3wwurd",
    "outputId": "8ee25c45-794d-4555-e1b3-1f28e2832941"
   },
   "outputs": [],
   "source": [
    "# Extraccion de un sample para poder manejarlo\n",
    "user_ratings = train_set.sample(5000)\n",
    "\n",
    "# Informacion del dataset\n",
    "user_matrix = user_ratings.pivot_table(columns='BGGId', index='Username', values='Rating')\n",
    "user_id_set = set(user_ratings['Username'])\n",
    "user_id_indexes = user_matrix.index.values\n",
    "user_matrix = user_matrix.fillna(0)\n",
    "numpy_array = user_matrix.to_numpy()\n",
    "sim_matrix = np.corrcoef(numpy_array)\n",
    "\n",
    "#Creacion del generador\n",
    "grpGenerator = GroupsGenerator.getGroupsGenerator(group_similarity_to_create)\n",
    "grpList = grpGenerator.generateGroups(user_id_indexes, user_id_set, sim_matrix, group_sizes_to_create, group_number)\n",
    "\n",
    "#display(pd.DataFrame.from_records(grpList))\n",
    "pd.DataFrame.from_records(grpList).to_csv('synthetic_groups.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhdLEKTqLXKj"
   },
   "source": [
    "# Evaluación de baselines para 1 usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "0g_ntaRqxwgK",
    "outputId": "315707fc-ae37-47a6-a865-16c8b100a0ab"
   },
   "outputs": [],
   "source": [
    "# Trabajaremos con un top 10\n",
    "top_n = 10\n",
    "test_set.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgqznCEGq2w7",
    "outputId": "7f56557f-f017-4ba3-a5a0-5650d16b4194"
   },
   "outputs": [],
   "source": [
    "# Revisemos el tamaño del dataset para asegurarnos de que tiene un tamaño trabajable:\n",
    "print(\"Tamaño del dataset completo:\", user_ratings.shape)\n",
    "print(\"Tamaño del dataset de entrenamiento:\", train_set.shape)\n",
    "print(\"Tamaño del dataset de prueba:\", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "4rF526_4zGS7",
    "outputId": "d6a78f8b-b0d5-4a32-b15b-a4686f962a03"
   },
   "outputs": [],
   "source": [
    "# Evaluamos UserKNN\n",
    "myUserKnn = pyreclab.UserKnn(dataset='train.csv', dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1)\n",
    "myUserKnn.train(k=7, similarity='pearson')\n",
    "_, maeUK, rmseUK = myUserKnn.test(input_file = 'test.csv', dlmchar = b',', header = False, usercol = 2, itemcol = 0, ratingcol = 1)\n",
    "_, mapUK, ndcgUK = myUserKnn.testrec(input_file=\"test.csv\", dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1, topn=top_n)\n",
    "\n",
    "print(f\"mae = {maeUK} y rmse = {rmseUK}\")\n",
    "print(f\"map = {mapUK} y ndcg = {ndcgUK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUPkB3E4v1fo",
    "outputId": "69b494c9-67d0-4110-f8f1-2bca5084cbcb"
   },
   "outputs": [],
   "source": [
    "# Evaluamos ItemKNN\n",
    "myItemKnn = pyreclab.ItemKnn(dataset='train.csv', dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1)\n",
    "myItemKnn.train(k=7, similarity='pearson')\n",
    "_, maeIK, rmseIK = myItemKnn.test(input_file = 'test.csv', dlmchar = b',', header = False, usercol = 2, itemcol = 0, ratingcol = 1)\n",
    "_, mapIK, ndcgIK = myItemKnn.testrec(input_file=\"test.csv\", dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1, topn=top_n)\n",
    "\n",
    "print(f\"mae = {maeIK} y rmse = {rmseIK}\")\n",
    "print(f\"map = {mapIK} y ndcg = {ndcgIK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7L7aVu-qzRY_",
    "outputId": "023604bd-526e-44ef-e63a-533c21ae19e2"
   },
   "outputs": [],
   "source": [
    "# Evaluamos SVD\n",
    "mySVD = pyreclab.SVD(dataset='train.csv', dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1)\n",
    "mySVD.train(factors=50, maxiter=80, lr=0.1, lamb=0.5)\n",
    "_, maeSVD, rmseSVD = mySVD.test(input_file = 'test.csv', dlmchar = b',', header = False, usercol = 2, itemcol = 0, ratingcol = 1)\n",
    "_, mapSVD, ndcgSVD = mySVD.testrec(input_file=\"test.csv\", dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1, topn=top_n)\n",
    "\n",
    "print(f\"mae = {maeSVD} y rmse = {rmseSVD}\")\n",
    "print(f\"map = {mapSVD} y ndcg = {ndcgSVD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efI8FJ00VMF2",
    "outputId": "0bebd840-5764-47ec-a765-ce0d53de9320"
   },
   "outputs": [],
   "source": [
    "# Evaluamos Most Popular\n",
    "myMP = pyreclab.MostPopular(dataset='train.csv', dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1)\n",
    "myMP.train(progress=False)\n",
    "_, mapMP, ndcgMP = myMP.testrec(input_file=\"test.csv\", dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1, topn=top_n)\n",
    "\n",
    "print(f\"map = {mapMP} y ndcg = {ndcgMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofL_ECIkRH-r",
    "outputId": "2a24baa8-e89d-4f97-8fef-bd99c59f2e8c"
   },
   "outputs": [],
   "source": [
    "# Evaluamos Random ratings\n",
    "predictions = []\n",
    "\n",
    "rating_scale = (1, 10)\n",
    "\n",
    "for _, row in test_set.iterrows():\n",
    "    itemId = row[\"BGGId\"]; rating = row[\"Rating\"]; userId = row[\"Username\"]\n",
    "    random_rating = random.uniform(rating_scale[0], rating_scale[1])\n",
    "    predictions.append((userId, itemId, rating, random_rating, None))\n",
    "\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqp0_9BFq2w8"
   },
   "source": [
    "# Recomendación multimodal para un usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naFpdN49q2w8"
   },
   "source": [
    "Setup del metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiMiS3dwq2w8",
    "outputId": "09145191-786f-44f2-91d8-a89e7d215a42"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0TYGEdLq2w8"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = train_set\n",
    "cols_with_id = {col: idx for idx, col in enumerate(mechanics_df.columns[1:])}\n",
    "\n",
    "# Conjunto de features\n",
    "item_styles = {}\n",
    "\n",
    "for _, row in train_set.iterrows():\n",
    "    bgg_id = row['BGGId']\n",
    "    style_row = mechanics_df[mechanics_df['BGGId'] == bgg_id].drop(columns=['BGGId'])\n",
    "    styles = style_row.columns[style_row.iloc[0] == 1].tolist()\n",
    "    item_styles[bgg_id] = styles\n",
    "\n",
    "print(item_styles)\n",
    "\n",
    "\n",
    "\n",
    "itemslist = df['BGGId'].unique()\n",
    "userslist = df['Username'].unique()\n",
    "stylelist = [i for i in range(len(cols_with_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conjunto de features, pero numericos\n",
    "\n",
    "item_styles_with_ids = {}\n",
    "for item_id, styles in item_styles.items():\n",
    "    style_ids = [cols_with_id[style] for style in styles]\n",
    "    item_styles_with_ids[item_id] = style_ids\n",
    "print(item_styles_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = [(row['Username'], row['BGGId'], row['Rating']) for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "all_features = set(feature for features in item_styles.values() for feature in features)\n",
    "\n",
    "dataset.fit(users=userslist, items=itemslist, item_features=all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(interactions_matrix, weights_matrix) = dataset.build_interactions(\n",
    "    ((x[0], x[1], x[2]) for x in interactions)\n",
    ")\n",
    "\n",
    "item_features = dataset.build_item_features(\n",
    "    ((item_id, features) for item_id, features in item_styles.items())\n",
    ")\n",
    "print(interactions_matrix)\n",
    "# print(item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(no_components=30, loss='warp')\n",
    "model.fit(interactions_matrix, item_features=item_features, epochs=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(model, dataset, user_ids, n_items=5):\n",
    "    n_users, n_items_total = interactions_matrix.shape\n",
    "    item_ids = np.arange(n_items_total)\n",
    "    recommendations_per_user = {}\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        scores = model.predict(user_id, item_ids, item_features=item_features)\n",
    "        top_items = item_ids[np.argsort(-scores)][:n_items]\n",
    "        item_mapping = dataset.mapping()[2]\n",
    "        item_id_mapping = {v: k for k, v in item_mapping.items()}\n",
    "        recommended_items = [item_id_mapping[item] for item in top_items]\n",
    "        print(f\"User {user_id} recommended items: {recommended_items}\")\n",
    "        recommendations_per_user[user_id] = recommended_items\n",
    "    \n",
    "    return recommendations_per_user\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision = precision_at_k(model, interactions_matrix, item_features=item_features, k=5).mean()\n",
    "train_recall = recall_at_k(model, interactions_matrix, item_features=item_features, k=5).mean()\n",
    "\n",
    "print(f'Train precision at k: {train_precision}')\n",
    "print(f'Train Recall: {train_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_group = recommend(model, dataset, [1, 2, 3], n_items=1000)\n",
    "print(type(userslist))\n",
    "print(recommendations_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4vgMrNHWxWl"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ponderación para grupos de usuarios con recomendaciones de metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_df = pd.read_csv('synthetic_groups.csv')\n",
    "groups_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código adaptado de repositorio mencionado anteriormente, en este caso tomamos los items que en promedio son más preferidos para recomendarlos al grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_group(model, dataset, group_members, n_items=5):\n",
    "    n_users, n_items_total = interactions_matrix.shape\n",
    "    item_ids = np.arange(n_items_total)\n",
    "    user_mapping = dataset.mapping()[0]\n",
    "    \n",
    "    group_scores = []\n",
    "    for user_id in group_members:\n",
    "        try:\n",
    "            internal_user_id = user_mapping[user_id]\n",
    "            scores = model.predict(internal_user_id, item_ids, item_features=item_features)\n",
    "            group_scores.append(scores)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if not group_scores:\n",
    "        return []\n",
    "    \n",
    "    average_scores = np.mean(group_scores, axis=0)\n",
    "    top_items = item_ids[np.argsort(-average_scores)][:n_items]\n",
    "    \n",
    "    item_mapping = dataset.mapping()[2]\n",
    "    item_id_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    recommended_items = [item_id_mapping[item] for item in top_items]\n",
    "    \n",
    "    return recommended_items\n",
    "\n",
    "group_recommendations = []\n",
    "for _, row in groups_df.iterrows():\n",
    "    group_members = eval(row['group_members']) if isinstance(row['group_members'], str) else row['group_members']\n",
    "    recommendations = recommend_for_group(model, dataset, group_members, n_items=10)\n",
    "    group_recommendations.append({\n",
    "        'group_members': group_members,\n",
    "        'recommendations': recommendations\n",
    "    })\n",
    "\n",
    "recommendations_df = pd.DataFrame(group_recommendations)\n",
    "recommendations_df.to_csv('group_recommendations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_group_multiple_models(model_name, model, group_members, n_items=10, include_rated=False):\n",
    "    \"\"\"\n",
    "    Generate recommendations for a group using different pyreclab models\n",
    "    Following exact format of original recommend_for_group function\n",
    "    \"\"\"\n",
    "    group_scores = []\n",
    "    \n",
    "    \n",
    "    for user_id in group_members:\n",
    "        print(f\"Recomendacion para user {user_id}\")\n",
    "        try:\n",
    "            ranking = model.recommend(str(user_id), n_items, include_rated)\n",
    "      \n",
    "            recommended_items = ranking\n",
    "            group_scores.append(recommended_items)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting recommendations for user {user_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not group_scores:\n",
    "        return []\n",
    "    \n",
    "    item_counts = {}\n",
    "    for user_recs in group_scores:\n",
    "        for item in user_recs:\n",
    "            item_counts[item] = item_counts.get(item, 0) + 1\n",
    "    \n",
    "    sorted_items = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    recommendations = [int(item) for item, _ in sorted_items[:n_items]]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "models = {\n",
    "    'SVD': mySVD,\n",
    "    # 'UserKNN': myUserKnn,\n",
    "    # 'ItemKNN': myItemKnn,\n",
    "    'MostPopular': myMP\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nGenerating recommendations using {model_name}...\")\n",
    "    group_recommendations = []\n",
    "    \n",
    "    for _, row in groups_df.head(1000).iterrows():\n",
    "        group_members = ast.literal_eval(row['group_members']) if isinstance(row['group_members'], str) else row['group_members']\n",
    "        recommendations = recommend_for_group_multiple_models(model_name, model, group_members, n_items=10)\n",
    "        \n",
    "        group_recommendations.append({\n",
    "            'group_members': str(group_members),\n",
    "            'recommendations': str(recommendations)\n",
    "        })\n",
    "    \n",
    "    recommendations_df = pd.DataFrame(group_recommendations)\n",
    "    recommendations_df.to_csv(f'{model_name.lower()}_group_recommendations.csv', index=False)\n",
    "    print(f\"Generated recommendations for {len(recommendations_df)} groups\")\n",
    "    print(\"Sample format:\")\n",
    "    print(recommendations_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def parse_group_data(groups_df):\n",
    "    parsed_data = []\n",
    "    \n",
    "    for _, row in groups_df.head(30).iterrows():\n",
    "        group_members = ast.literal_eval(row['group_members'])\n",
    "        recommendations = ast.literal_eval(row['recommendations'])\n",
    "        parsed_data.append((group_members, recommendations))\n",
    "    \n",
    "    print(f\"\\nTotal groups parsed: {len(parsed_data)}\")\n",
    "    return parsed_data\n",
    "\n",
    "def calculate_relevance(train_df, user_id, item_id):\n",
    "    rating = train_df[\n",
    "        (train_df['Username'] == user_id) & \n",
    "        (train_df['BGGId'] == item_id)\n",
    "    ]['Rating']\n",
    "    \n",
    "    return rating.iloc[0] if not rating.empty else 0\n",
    "\n",
    "def calculate_group_metrics(parsed_data, train_df, k=5, threshold=3.5):\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_ndcg = 0\n",
    "    total_relevance = 0\n",
    "    total_groups = len(parsed_data)\n",
    "    total_hits = 0\n",
    "    total_possible_hits = 0\n",
    "    all_group_sizes = []\n",
    "    \n",
    "    for i, (group_members, recommendations) in enumerate(parsed_data):\n",
    "        print(f\"\\rProcessing Group {i+1}/{total_groups}\", end=\"\")\n",
    "        recommended_items = recommendations[:k]\n",
    "        group_relevance = []\n",
    "        group_hits = 0\n",
    "        for item_id in recommended_items:\n",
    "            member_ratings = []\n",
    "            for user_id in group_members:\n",
    "                rating = calculate_relevance(train_df, user_id, item_id)\n",
    "                if rating > 0:\n",
    "                    member_ratings.append(rating)\n",
    "                    group_hits += 1\n",
    "                total_possible_hits += 1\n",
    "            avg_rating = np.mean(member_ratings) if member_ratings else 0\n",
    "            group_relevance.append(avg_rating)\n",
    "        \n",
    "        total_hits += group_hits\n",
    "        \n",
    "        binary_relevance = [1 if rel >= threshold else 0 for rel in group_relevance]\n",
    "        \n",
    "        total_relevant = 0\n",
    "        unique_items = train_df['BGGId'].unique()\n",
    "        for item_id in unique_items:\n",
    "            member_ratings = []\n",
    "            for user_id in group_members:\n",
    "                rating = calculate_relevance(train_df, user_id, item_id)\n",
    "                if rating > 0:\n",
    "                    member_ratings.append(rating)\n",
    "            avg_rating = np.mean(member_ratings) if member_ratings else 0\n",
    "            if avg_rating >= threshold:\n",
    "                total_relevant += 1\n",
    "        \n",
    "        precision = sum(binary_relevance) / k if k > 0 else 0\n",
    "        recall = sum(binary_relevance) / total_relevant if total_relevant > 0 else 0\n",
    "        \n",
    "        dcg = sum((2 ** rel - 1) / np.log2(idx + 2) \n",
    "                  for idx, rel in enumerate(group_relevance))\n",
    "        ideal_relevance = sorted(group_relevance, reverse=True)\n",
    "        idcg = sum((2 ** rel - 1) / np.log2(idx + 2) \n",
    "                   for idx, rel in enumerate(ideal_relevance))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        \n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_ndcg += ndcg\n",
    "        total_relevance += np.mean(group_relevance) if group_relevance else 0\n",
    "    \n",
    "    avg_metrics = {\n",
    "        \"average_precision\": total_precision / total_groups,\n",
    "        \"average_recall\": total_recall / total_groups,\n",
    "        \"average_ndcg\": total_ndcg / total_groups,\n",
    "        \"average_relevance\": total_relevance / total_groups,\n",
    "        # \"hit_rate\": total_hits / total_possible_hits,\n",
    "        # \"average_group_size\": np.mean(all_group_sizes),\n",
    "        # \"total_groups\": total_groups,\n",
    "        # \"total_hits\": total_hits,\n",
    "        # \"total_possible_hits\": total_possible_hits\n",
    "    }\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Requirement already satisfied: cudf-cu12 in /home/nico/.local/lib/python3.10/site-packages (24.10.1)\n",
      "Requirement already satisfied: rich in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (13.7.1)\n",
      "Requirement already satisfied: pandas<2.2.3dev0,>=2.0 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (2.2.1)\n",
      "Requirement already satisfied: pylibcudf-cu12==24.10.* in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (24.10.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (2024.6.1)\n",
      "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (13.3.0)\n",
      "Requirement already satisfied: numpy<3.0a0,>=1.23 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (1.26.4)\n",
      "Requirement already satisfied: numba>=0.57 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (0.60.0)\n",
      "Requirement already satisfied: pyarrow<18.0.0a0,>=14.0.0 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (16.1.0)\n",
      "Requirement already satisfied: rmm-cu12==24.10.* in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (24.10.0)\n",
      "Requirement already satisfied: nvtx>=0.2.1 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (0.2.10)\n",
      "Requirement already satisfied: packaging in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (24.0)\n",
      "Requirement already satisfied: pynvjitlink-cu12 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (0.4.0)\n",
      "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (12.6.2.post1)\n",
      "Requirement already satisfied: cachetools in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (5.5.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (4.10.0)\n",
      "Requirement already satisfied: libcudf-cu12==24.10.* in /home/nico/.local/lib/python3.10/site-packages (from cudf-cu12) (24.10.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /home/nico/.local/lib/python3.10/site-packages (from cupy-cuda12x>=12.0.0->cudf-cu12) (0.8.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/nico/.local/lib/python3.10/site-packages (from numba>=0.57->cudf-cu12) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nico/.local/lib/python3.10/site-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nico/.local/lib/python3.10/site-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nico/.local/lib/python3.10/site-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12) (2.8.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nico/.local/lib/python3.10/site-packages (from rich->cudf-cu12) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nico/.local/lib/python3.10/site-packages (from rich->cudf-cu12) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/nico/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.3dev0,>=2.0->cudf-cu12) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Para que esto funcione tienes que tener instalado CUDA!!!\n",
    "# Lo puedes instalar para WSL acá: https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=WSL-Ubuntu&target_version=2.0&target_type=runfile_local\n",
    "!pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12\n",
    "# Esto debería demorar ~7 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "# verificación\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcudart.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcudf.pandas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_str:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing module name.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malready loaded\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m extension is already loaded. To reload it, use:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m module_str)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/extensions.py:62\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/extensions.py:77\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m---> 77\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     mod \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[module_str]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_load_ipython_extension(mod):\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cudf/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpu_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_setup\n\u001b[1;32m     19\u001b[0m _setup_numba()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mvalidate_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m numba_config, cuda\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cudf/utils/gpu_utils.py:96\u001b[0m, in \u001b[0;36mvalidate_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     minor_version \u001b[38;5;241m=\u001b[39m getDeviceAttribute(\n\u001b[1;32m     87\u001b[0m         cudaDeviceAttr\u001b[38;5;241m.\u001b[39mcudaDevAttrComputeCapabilityMinor, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedCUDAError(\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA GPU with NVIDIA Volta™ (Compute Capability 7.0) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor newer architecture is required.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected GPU 0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected Compute Capability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmajor_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminor_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m cuda_runtime_version \u001b[38;5;241m=\u001b[39m \u001b[43mruntimeGetVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cuda_runtime_version \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m11000\u001b[39m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Require CUDA Runtime version 11.0 or greater.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     major_version \u001b[38;5;241m=\u001b[39m cuda_runtime_version \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/rmm/_cuda/gpu.py:88\u001b[0m, in \u001b[0;36mruntimeGetVersion\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# TODO: Replace this with `cuda.cudart.cudaRuntimeGetVersion()` when the\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# limitation is fixed.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m major, minor \u001b[38;5;241m=\u001b[39m \u001b[43mnumba\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m major \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m+\u001b[39m minor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/runtime.py:111\u001b[0m, in \u001b[0;36mRuntime.get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03mReturns the CUDA Runtime version as a tuple (major, minor).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m rtver \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudaRuntimeGetVersion\u001b[49m(ctypes\u001b[38;5;241m.\u001b[39mbyref(rtver))\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# The version is encoded as (1000 * major) + (10 * minor)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m major \u001b[38;5;241m=\u001b[39m rtver\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/runtime.py:65\u001b[0m, in \u001b[0;36mRuntime.__getattr__\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     62\u001b[0m argtypes \u001b[38;5;241m=\u001b[39m proto[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_initialized:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Find function in runtime library\u001b[39;00m\n\u001b[1;32m     68\u001b[0m libfn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_api(fname)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/runtime.py:51\u001b[0m, in \u001b[0;36mRuntime._initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is disabled due to setting NUMBA_DISABLE_CUDA=1 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the environment, or because CUDA is unsupported on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32-bit systems.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CudaSupportError(msg)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib \u001b[38;5;241m=\u001b[39m \u001b[43mopen_cudalib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcudart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/libs.py:65\u001b[0m, in \u001b[0;36mopen_cudalib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_cudalib\u001b[39m(lib):\n\u001b[1;32m     64\u001b[0m     path \u001b[38;5;241m=\u001b[39m get_cudalib(lib)\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libcudart.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "%load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "groups_df = pd.read_csv('group_recommendations.csv')\n",
    "parsed_data = parse_group_data(groups_df)\n",
    "metrics = calculate_group_metrics(parsed_data, train_df, k=5)\n",
    "print(f\"Precision: {metrics['average_precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['average_recall']:.3f}\")\n",
    "print(f\"nDCG: {metrics['average_ndcg']:.3f}\")\n",
    "print(f\"Relevance Score: {metrics['average_relevance']:.2f}\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "groups_df = pd.read_csv('svd_group_recommendations.csv')\n",
    "parsed_data = parse_group_data(groups_df)\n",
    "metrics = calculate_group_metrics(parsed_data, train_df, k=5)\n",
    "print(f\"Precision: {metrics['average_precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['average_recall']:.3f}\")\n",
    "print(f\"nDCG: {metrics['average_ndcg']:.3f}\")\n",
    "print(f\"Relevance Score: {metrics['average_relevance']:.2f}\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "groups_df = pd.read_csv('mostpopular_group_recommendations.csv')\n",
    "parsed_data = parse_group_data(groups_df)\n",
    "metrics = calculate_group_metrics(parsed_data, train_df, k=5)\n",
    "print(f\"Precision: {metrics['average_precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['average_recall']:.3f}\")\n",
    "print(f\"nDCG: {metrics['average_ndcg']:.3f}\")\n",
    "print(f\"Relevance Score: {metrics['average_relevance']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasta aca esta probado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell for AGREE implementation\n",
    "# New cell for AGREE implementation\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from scipy import sparse\n",
    "\n",
    "# Model classes\n",
    "class UserEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim):\n",
    "        super(UserEmbeddingLayer, self).__init__()\n",
    "        self.userEmbedding = nn.Embedding(num_users, embedding_dim)\n",
    "    \n",
    "    def forward(self, user_inputs):\n",
    "        return self.userEmbedding(user_inputs)\n",
    "\n",
    "class ItemEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim):\n",
    "        super(ItemEmbeddingLayer, self).__init__()\n",
    "        self.itemEmbedding = nn.Embedding(num_items, embedding_dim)\n",
    "    \n",
    "    def forward(self, item_inputs):\n",
    "        return self.itemEmbedding(item_inputs)\n",
    "\n",
    "class GroupEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_groups, embedding_dim):\n",
    "        super(GroupEmbeddingLayer, self).__init__()\n",
    "        self.groupEmbedding = nn.Embedding(num_groups, embedding_dim)\n",
    "    \n",
    "    def forward(self, group_inputs):\n",
    "        return self.groupEmbedding(group_inputs)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0.1):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_ratio),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        weight = torch.softmax(out.view(1, -1), dim=1)\n",
    "        return weight\n",
    "\n",
    "class PredictLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0.1):\n",
    "        super(PredictLayer, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_ratio),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class AGREE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_groups, embedding_dim, group_member_dict, drop_ratio=0.1):\n",
    "        super(AGREE, self).__init__()\n",
    "        self.userembeds = UserEmbeddingLayer(num_users, embedding_dim)\n",
    "        self.itemembeds = ItemEmbeddingLayer(num_items, embedding_dim)\n",
    "        self.groupembeds = GroupEmbeddingLayer(num_groups, embedding_dim)\n",
    "        self.attention = AttentionLayer(2 * embedding_dim, drop_ratio)\n",
    "        self.predictlayer = PredictLayer(3 * embedding_dim, drop_ratio)\n",
    "        self.group_member_dict = group_member_dict\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight)\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "    \n",
    "    def forward(self, group_inputs, user_inputs, item_inputs):\n",
    "        if (group_inputs is not None) and (user_inputs is None):\n",
    "            return self.group_forward(group_inputs, item_inputs)\n",
    "        else:\n",
    "            return self.user_forward(user_inputs, item_inputs)\n",
    "    \n",
    "    def group_forward(self, group_inputs, item_inputs):\n",
    "        group_embeds = torch.Tensor()\n",
    "        item_embeds_full = self.itemembeds(torch.LongTensor(item_inputs))\n",
    "        \n",
    "        for i, j in zip(group_inputs, item_inputs):\n",
    "            members = self.group_member_dict[i.item()]\n",
    "            members_embeds = self.userembeds(torch.LongTensor(members))\n",
    "            items_numb = [j] * len(members)\n",
    "            item_embeds = self.itemembeds(torch.LongTensor(items_numb))\n",
    "            \n",
    "            group_item_embeds = torch.cat((members_embeds, item_embeds), dim=1)\n",
    "            at_wt = self.attention(group_item_embeds)\n",
    "            g_embeds_with_attention = torch.matmul(at_wt, members_embeds)\n",
    "            group_embeds_pure = self.groupembeds(torch.LongTensor([i]))\n",
    "            g_embeds = g_embeds_with_attention + group_embeds_pure\n",
    "            group_embeds = torch.cat((group_embeds, g_embeds))\n",
    "        \n",
    "        element_embeds = torch.mul(group_embeds, item_embeds_full)\n",
    "        new_embeds = torch.cat((element_embeds, group_embeds, item_embeds_full), dim=1)\n",
    "        return torch.sigmoid(self.predictlayer(new_embeds))\n",
    "    \n",
    "    def user_forward(self, user_inputs, item_inputs):\n",
    "        user_embeds = self.userembeds(user_inputs)\n",
    "        item_embeds = self.itemembeds(item_inputs)\n",
    "        element_embeds = torch.mul(user_embeds, item_embeds)\n",
    "        new_embeds = torch.cat((element_embeds, user_embeds, item_embeds), dim=1)\n",
    "        return torch.sigmoid(self.predictlayer(new_embeds))\n",
    "\n",
    "# Load and prepare data\n",
    "groups_df = pd.read_csv('synthetic_groups.csv').iloc[:10000]\n",
    "groups_df['group_members'] = groups_df['group_members'].apply(literal_eval)\n",
    "\n",
    "# Convert sparse matrix to dense or CSR format for indexing\n",
    "if sparse.issparse(interactions_matrix):\n",
    "    interactions_matrix = interactions_matrix.tocsr()\n",
    "\n",
    "# Create mappings and initialize model\n",
    "all_users = set()\n",
    "for members in groups_df['group_members']:\n",
    "    all_users.update(members)\n",
    "\n",
    "user_to_idx = {old_id: idx for idx, old_id in enumerate(sorted(all_users))}\n",
    "item_to_idx = {old_id: idx for idx, old_id in enumerate(range(interactions_matrix.shape[1]))}\n",
    "group_member_dict = {\n",
    "    group_id: [user_to_idx[user] for user in members]\n",
    "    for group_id, members in enumerate(groups_df['group_members'])\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 32\n",
    "model = AGREE(\n",
    "    num_users=len(user_to_idx),\n",
    "    num_items=len(item_to_idx),\n",
    "    num_groups=len(groups_df),\n",
    "    embedding_dim=embedding_dim,\n",
    "    group_member_dict=group_member_dict\n",
    ")\n",
    "\n",
    "# Train model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(5):  # 5 epochs as example\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Train on individual interactions\n",
    "    for user_idx in range(interactions_matrix.shape[0]):\n",
    "        # Get non-zero elements for this user\n",
    "        items = interactions_matrix[user_idx].nonzero()[1]\n",
    "        \n",
    "        for item_idx in items:\n",
    "            user_tensor = torch.LongTensor([user_idx])\n",
    "            item_tensor = torch.LongTensor([item_idx])\n",
    "            \n",
    "            prediction = model(None, user_tensor, item_tensor)\n",
    "            target = torch.FloatTensor([[1.0]])  # Changed to 2D tensor\n",
    "            \n",
    "            loss = criterion(prediction, target)\n",
    "            \n",
    "            # Negative sampling\n",
    "            neg_item_idx = np.random.randint(0, interactions_matrix.shape[1])\n",
    "            while interactions_matrix[user_idx, neg_item_idx] != 0:\n",
    "                neg_item_idx = np.random.randint(0, interactions_matrix.shape[1])\n",
    "            \n",
    "            neg_item_tensor = torch.LongTensor([neg_item_idx])\n",
    "            neg_prediction = model(None, user_tensor, neg_item_tensor)\n",
    "            neg_target = torch.FloatTensor([[0.0]])  # Changed to 2D tensor\n",
    "            \n",
    "            loss += criterion(neg_prediction, neg_target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/5, Loss: {total_loss}')\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Function to get recommendations for a group\n",
    "def get_group_recommendations(model, group_idx, num_items, top_k=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = []\n",
    "        group_tensor = torch.LongTensor([group_idx])\n",
    "        \n",
    "        for item_idx in range(num_items):\n",
    "            item_tensor = torch.LongTensor([item_idx])\n",
    "            score = model(group_tensor, None, item_tensor)\n",
    "            scores.append((item_idx, score.item()))\n",
    "        \n",
    "        # Get top-k recommendations\n",
    "        recommendations = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        return [item_idx for item_idx, _ in recommendations]\n",
    "\n",
    "# Get recommendations for first group\n",
    "group_idx = 0\n",
    "recommendations = get_group_recommendations(model, group_idx, interactions_matrix.shape[1])\n",
    "print(f\"\\nTop 10 recommendations for group {group_idx}:\")\n",
    "print(recommendations)\n",
    "\n",
    "# Optional: Get recommendations for all groups\n",
    "print(\"\\nGenerating recommendations for all groups...\")\n",
    "all_group_recommendations = {}\n",
    "for group_idx in range(len(groups_df)):\n",
    "    recommendations = get_group_recommendations(model, group_idx, interactions_matrix.shape[1])\n",
    "    all_group_recommendations[group_idx] = recommendations\n",
    "    print(f\"Group {group_idx}: {recommendations}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
