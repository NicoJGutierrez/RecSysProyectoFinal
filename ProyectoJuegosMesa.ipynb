{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jH8fyVEjke2x"
   },
   "source": [
    "# Propuesta Proyecto RecSys: Recomendación Grupal de Juegos de Mesa (setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygicq0KDTnqq"
   },
   "source": [
    "Link de dataset: https://www.kaggle.com/datasets/threnjen/board-games-database-from-boardgamegeek?select=games.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMyiOuV3TyEZ",
    "outputId": "c7b45953-faab-45cb-c214-110509412021"
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!pip install pyreclab\n",
    "!pip install implicit\n",
    "!pip install surprise\n",
    "!pip install elliot\n",
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzALElwIUwwS"
   },
   "source": [
    "Tutorial usado: https://www.kaggle.com/discussions/general/74235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu7OpwbIW4Hu"
   },
   "source": [
    "Tutorial adicional usado: https://www.youtube.com/watch?v=yEXkEUqK52Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8V2_lleq2w5",
    "outputId": "bc5cc37a-a6fd-454f-8565-5331a610e4ad"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"threnjen/board-games-database-from-boardgamegeek\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xKgz0X2q2w6"
   },
   "source": [
    "Arriba de esto aparecerá un \"path to dataset files\", ese path se debe copiar y pegar en la línea de abajo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Y1R8PyNW2pp"
   },
   "outputs": [],
   "source": [
    "path_to_dataset_files = path\n",
    "#path_to_dataset_files = '/home/nico/.cache/kagglehub/datasets/threnjen/board-games-database-from-boardgamegeek/versions/4'\n",
    "#path_to_dataset_files = '/root/.cache/kagglehub/datasets/threnjen/board-games-database-from-boardgamegeek/versions/4'\n",
    "\n",
    "import os\n",
    "# Guardamos el directorio actual\n",
    "base_dir = os.getcwd()\n",
    "# Cambiamos al directorio donde se encuentra el dataset\n",
    "os.chdir(path_to_dataset_files)\n",
    "\n",
    "# Importamos las librerias\n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreclab\n",
    "import tempfile\n",
    "import implicit\n",
    "import random\n",
    "from surprise import accuracy\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZBjZKWDK2Uq"
   },
   "source": [
    "Generamos los datos a utilizar como un muestreo del dataset original pues es muy grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX_VaAH9Xs-T",
    "outputId": "c7505886-1461-4785-b812-d0a970744e33"
   },
   "outputs": [],
   "source": [
    "# Leemos el csv y volvemos al directorio base del proyecto\n",
    "user_ratings = pd.read_csv('user_ratings.csv')\n",
    "mechanics_df = pd.read_csv('mechanics.csv')\n",
    "# Volvemos al directorio base del proyecto\n",
    "os.chdir(base_dir)\n",
    "print(base_dir)\n",
    "\n",
    "# Cambiamos username por un userid\n",
    "a=list(set(list(user_ratings.Username)))\n",
    "d = {a[i]: i for i in range(len(a))}\n",
    "a_mod = [d[i] for i in list(user_ratings.Username)]\n",
    "user_ratings[\"Username\"] = a_mod\n",
    "\n",
    "# Separamos training y testing\n",
    "train     = list(set(user_ratings.Username))[:8000]\n",
    "test      = list(set(user_ratings.Username))[8000:11000]\n",
    "test_set  = user_ratings[user_ratings[\"Username\"].isin(test)].sample(9000)\n",
    "train_set = user_ratings[user_ratings[\"Username\"].isin(train)].sample(35000)\n",
    "\n",
    "# Generamos nuevo csv de training y testing\n",
    "train_set.to_csv(\"train.csv\", index=False, sep=',', header=True)\n",
    "test_set.to_csv(\"test.csv\", index=False, sep=',', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vcXoaD-wYcB"
   },
   "source": [
    "# Creación de grupos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código importado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pni1vfFjwcCX"
   },
   "outputs": [],
   "source": [
    "# Groups generator from: https://github.com/barnap/group-recommenders-offline-evaluation/blob/main/synthetic_groups_generation/groups_generators.py\n",
    "\n",
    "class GroupsGenerator(ABC):\n",
    "\n",
    "    @staticmethod\n",
    "    def getGroupsGenerator(type):\n",
    "        if type == \"RANDOM\":\n",
    "            return RandomGroupsGenerator()\n",
    "        elif type == \"SIMILAR\":\n",
    "            return SimilarGroupsGenerator()\n",
    "        elif type == \"DIVERGENT\":\n",
    "            return DivergentGroupsGenerator()\n",
    "        elif type == \"SIMILAR_ONE_DIVERGENT\":\n",
    "            return MinorityGroupsGenerator()\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_average_similarity(group, user_id_indexes, sim_matrix):\n",
    "        similarities = list()\n",
    "        for user_1 in group:\n",
    "            user_1_index = user_id_indexes.tolist().index(user_1)\n",
    "            for user_2 in group:\n",
    "                user_2_index = user_id_indexes.tolist().index(user_2)\n",
    "                if user_1 != user_2:\n",
    "                    similarities.append(sim_matrix[user_1_index][user_2_index])\n",
    "        return np.mean(similarities)\n",
    "\n",
    "    @abstractmethod\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomGroupsGenerator(GroupsGenerator):\n",
    "\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        groups_list = list()\n",
    "        for group_size in group_sizes_to_create:\n",
    "            for i in range(group_number_to_create):\n",
    "                group = random.sample(user_id_set, group_size)\n",
    "                groups_list.append(\n",
    "                    {\n",
    "                        \"group_size\": group_size,\n",
    "                        \"group_similarity\": 'random',\n",
    "                        \"group_members\": group,\n",
    "                        \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n",
    "                    }\n",
    "                )\n",
    "            print(len(groups_list))\n",
    "        return groups_list\n",
    "\n",
    "\n",
    "class SimilarGroupsGenerator(GroupsGenerator):\n",
    "\n",
    "    @staticmethod\n",
    "    def select_user_for_sim_group(group, sim_matrix, user_id_indexes, sim_threshold=0.4):\n",
    "        '''\n",
    "        Helper function to the generate_similar_user_groups function. Given already selected group members, it randomly\n",
    "        selects from the remaining users that has a PCC value >= sim_threshold to any of the existing members.\n",
    "        :param group:\n",
    "        :param sim_matrix:\n",
    "        :param user_id_indexes:\n",
    "        :param sim_threshold:\n",
    "        :return:\n",
    "        '''\n",
    "        ids_to_select_from = set()\n",
    "        for member in group:\n",
    "            member_index = user_id_indexes.tolist().index(member)\n",
    "            indexes = np.where(sim_matrix[member_index] >= sim_threshold)[0].tolist()\n",
    "            user_ids = [user_id_indexes[index] for index in indexes]\n",
    "            ids_to_select_from = ids_to_select_from.union(set(user_ids))\n",
    "        candidate_ids = ids_to_select_from.difference(set(group))\n",
    "        if len(candidate_ids) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            selection = random.sample(candidate_ids, 1)\n",
    "            return selection[0]\n",
    "\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        groups_list = list()\n",
    "        for group_size in group_sizes_to_create:\n",
    "            groups_size_list = list()\n",
    "            while (len(groups_size_list) < group_number_to_create):\n",
    "                group = random.sample(user_id_set, 1)\n",
    "                while len(group) < group_size:\n",
    "                    new_member = SimilarGroupsGenerator.select_user_for_sim_group(group, similarity_matrix,\n",
    "                                                                                  user_id_indexes,\n",
    "                                                                                  sim_threshold=0.5)\n",
    "                    if new_member is None:\n",
    "                        break\n",
    "                    group.append(new_member)\n",
    "                if len(group) == group_size:\n",
    "                    groups_size_list.append(\n",
    "                        {\n",
    "                            \"group_size\": group_size,\n",
    "                            \"group_similarity\": 'similar',\n",
    "                            \"group_members\": group,\n",
    "                            \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n",
    "                        }\n",
    "                    )\n",
    "            groups_list.extend(groups_size_list)\n",
    "            print(len(groups_list))\n",
    "        return groups_list\n",
    "\n",
    "\n",
    "class DivergentGroupsGenerator(GroupsGenerator):\n",
    "\n",
    "    @staticmethod\n",
    "    def select_user_for_divergent_group(group, sim_matrix, user_id_indexes, sim_threshold=0.0):\n",
    "        '''\n",
    "        Helper function to the generate_similar_user_groups function. Given already selected group members, it randomly\n",
    "        selects from the remaining users that has a PCC value < sim_threshold to any of the existing members.\n",
    "        :param group:\n",
    "        :param sim_matrix:\n",
    "        :param user_id_indexes:\n",
    "        :param sim_threshold:\n",
    "        :return:\n",
    "        '''\n",
    "        ids_to_select_from = set()\n",
    "        for member in group:\n",
    "            member_index = user_id_indexes.tolist().index(member)\n",
    "            indexes = np.where(sim_matrix[member_index] < sim_threshold)[0].tolist()\n",
    "            user_ids = [user_id_indexes[index] for index in indexes]\n",
    "            ids_to_select_from = ids_to_select_from.union(set(user_ids))\n",
    "        candidate_ids = ids_to_select_from.difference(set(group))\n",
    "        if len(candidate_ids) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            selection = random.sample(candidate_ids, 1)\n",
    "            return selection[0]\n",
    "\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        groups_list = list()\n",
    "        for group_size in group_sizes_to_create:\n",
    "            groups_size_list = list()\n",
    "            while (len(groups_size_list) < group_number_to_create):\n",
    "                group = random.sample(user_id_set, 1)\n",
    "                while len(group) < group_size:\n",
    "                    new_member = DivergentGroupsGenerator.select_user_for_divergent_group(group, similarity_matrix,\n",
    "                                                                                     user_id_indexes,\n",
    "                                                                                     sim_threshold=-0.1)\n",
    "                    if new_member is None:\n",
    "                        break\n",
    "                    group.append(new_member)\n",
    "                if len(group) == group_size:\n",
    "                    groups_size_list.append(\n",
    "                        {\n",
    "                            \"group_size\": group_size,\n",
    "                            \"group_similarity\": 'divergent',\n",
    "                            \"group_members\": group,\n",
    "                            \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n",
    "                        }\n",
    "                    )\n",
    "            groups_list.extend(groups_size_list)\n",
    "            print(len(groups_list))\n",
    "        return groups_list\n",
    "\n",
    "\n",
    "class MinorityGroupsGenerator(GroupsGenerator):\n",
    "    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n",
    "        groups_list = list()\n",
    "        for group_size in group_sizes_to_create:\n",
    "            groups_size_list = list()\n",
    "            while (len(groups_size_list) < group_number_to_create):\n",
    "                group = random.sample(user_id_set, 1)\n",
    "                while len(group) < (group_size - 1):\n",
    "                    new_member = SimilarGroupsGenerator.select_user_for_sim_group(group, similarity_matrix,\n",
    "                                                                                     user_id_indexes,\n",
    "                                                                                     sim_threshold=0.5)\n",
    "                    if new_member is None:\n",
    "                        break\n",
    "                    group.append(new_member)\n",
    "\n",
    "                dissimilar_member = DivergentGroupsGenerator.select_user_for_divergent_group(group, similarity_matrix,\n",
    "                                                                                              user_id_indexes,\n",
    "                                                                                              sim_threshold=-0.1)\n",
    "                if dissimilar_member is not None:\n",
    "                    group.append(dissimilar_member)\n",
    "                if len(group) == group_size:\n",
    "                    groups_size_list.append(\n",
    "                        {\n",
    "                            \"group_size\": group_size,\n",
    "                            \"group_similarity\": 'similar_one_divergent',\n",
    "                            \"group_members\": group,\n",
    "                            \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n",
    "                        }\n",
    "                    )\n",
    "            groups_list.extend(groups_size_list)\n",
    "            print(len(groups_list))\n",
    "        return groups_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código nuestro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh7Zuy7SN1QJ"
   },
   "source": [
    "Informacion para crear grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56xPcl69N0i6"
   },
   "outputs": [],
   "source": [
    "group_sizes_to_create = [4]        # [2, 3, 4, 5, 6, 7, 8]\n",
    "group_similarity_to_create = \"RANDOM\"  # [\"RANDOM\", \"SIMILAR\", \"DIVERGENT\", \"SIMILAR_ONE_DIVERGENT\"]\n",
    "group_number = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck2euynXwvOp"
   },
   "source": [
    "Creacion de los grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlVeyb3wwurd",
    "outputId": "8ee25c45-794d-4555-e1b3-1f28e2832941"
   },
   "outputs": [],
   "source": [
    "# Extraccion de un sample para poder manejarlo\n",
    "user_ratings = train_set.sample(5000)\n",
    "\n",
    "# Informacion del dataset\n",
    "user_matrix = user_ratings.pivot_table(columns='BGGId', index='Username', values='Rating')\n",
    "user_id_set = set(user_ratings['Username'])\n",
    "user_id_indexes = user_matrix.index.values\n",
    "user_matrix = user_matrix.fillna(0)\n",
    "numpy_array = user_matrix.to_numpy()\n",
    "sim_matrix = np.corrcoef(numpy_array)\n",
    "\n",
    "#Creacion del generador\n",
    "grpGenerator = GroupsGenerator.getGroupsGenerator(group_similarity_to_create)\n",
    "grpList = grpGenerator.generateGroups(user_id_indexes, user_id_set, sim_matrix, group_sizes_to_create, group_number)\n",
    "\n",
    "#display(pd.DataFrame.from_records(grpList))\n",
    "pd.DataFrame.from_records(grpList).to_csv('synthetic_groups.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhdLEKTqLXKj"
   },
   "source": [
    "# Evaluación de baselines para 1 usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "0g_ntaRqxwgK",
    "outputId": "315707fc-ae37-47a6-a865-16c8b100a0ab"
   },
   "outputs": [],
   "source": [
    "# Trabajaremos con un top 10\n",
    "top_n = 10\n",
    "test_set.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgqznCEGq2w7",
    "outputId": "7f56557f-f017-4ba3-a5a0-5650d16b4194"
   },
   "outputs": [],
   "source": [
    "# Revisemos el tamaño del dataset para asegurarnos de que tiene un tamaño trabajable:\n",
    "print(\"Tamaño del dataset completo:\", user_ratings.shape)\n",
    "print(\"Tamaño del dataset de entrenamiento:\", train_set.shape)\n",
    "print(\"Tamaño del dataset de prueba:\", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "4rF526_4zGS7",
    "outputId": "d6a78f8b-b0d5-4a32-b15b-a4686f962a03"
   },
   "outputs": [],
   "source": [
    "# Evaluamos UserKNN\n",
    "myUserKnn = pyreclab.UserKnn(dataset='train.csv', dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1)\n",
    "myUserKnn.train(k=7, similarity='pearson')\n",
    "_, maeUK, rmseUK = myUserKnn.test(input_file = 'test.csv', dlmchar = b',', header = False, usercol = 2, itemcol = 0, ratingcol = 1)\n",
    "_, mapUK, ndcgUK = myUserKnn.testrec(input_file=\"test.csv\", dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1, topn=top_n)\n",
    "\n",
    "print(f\"mae = {maeUK} y rmse = {rmseUK}\")\n",
    "print(f\"map = {mapUK} y ndcg = {ndcgUK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUPkB3E4v1fo",
    "outputId": "69b494c9-67d0-4110-f8f1-2bca5084cbcb"
   },
   "outputs": [],
   "source": [
    "# Evaluamos ItemKNN\n",
    "myItemKnn = pyreclab.ItemKnn(dataset='train.csv', dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1)\n",
    "myItemKnn.train(k=7, similarity='pearson')\n",
    "_, maeIK, rmseIK = myItemKnn.test(input_file = 'test.csv', dlmchar = b',', header = False, usercol = 2, itemcol = 0, ratingcol = 1)\n",
    "_, mapIK, ndcgIK = myItemKnn.testrec(input_file=\"test.csv\", dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1, topn=top_n)\n",
    "\n",
    "print(f\"mae = {maeIK} y rmse = {rmseIK}\")\n",
    "print(f\"map = {mapIK} y ndcg = {ndcgIK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7L7aVu-qzRY_",
    "outputId": "023604bd-526e-44ef-e63a-533c21ae19e2"
   },
   "outputs": [],
   "source": [
    "# Evaluamos SVD\n",
    "mySVD = pyreclab.SVD(dataset='train.csv', dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1)\n",
    "mySVD.train(factors=50, maxiter=80, lr=0.1, lamb=0.5)\n",
    "_, maeSVD, rmseSVD = mySVD.test(input_file = 'test.csv', dlmchar = b',', header = False, usercol = 2, itemcol = 0, ratingcol = 1)\n",
    "_, mapSVD, ndcgSVD = mySVD.testrec(input_file=\"test.csv\", dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1, topn=top_n)\n",
    "\n",
    "print(f\"mae = {maeSVD} y rmse = {rmseSVD}\")\n",
    "print(f\"map = {mapSVD} y ndcg = {ndcgSVD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efI8FJ00VMF2",
    "outputId": "0bebd840-5764-47ec-a765-ce0d53de9320"
   },
   "outputs": [],
   "source": [
    "# Evaluamos Most Popular\n",
    "myMP = pyreclab.MostPopular(dataset='train.csv', dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1)\n",
    "myMP.train(progress=False)\n",
    "_, mapMP, ndcgMP = myMP.testrec(input_file=\"test.csv\", dlmchar=b',', header=False, usercol=2, itemcol=0, ratingcol=1, topn=top_n)\n",
    "\n",
    "print(f\"map = {mapMP} y ndcg = {ndcgMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofL_ECIkRH-r",
    "outputId": "2a24baa8-e89d-4f97-8fef-bd99c59f2e8c"
   },
   "outputs": [],
   "source": [
    "# Evaluamos Random ratings\n",
    "predictions = []\n",
    "\n",
    "rating_scale = (1, 10)\n",
    "\n",
    "for _, row in test_set.iterrows():\n",
    "    itemId = row[\"BGGId\"]; rating = row[\"Rating\"]; userId = row[\"Username\"]\n",
    "    random_rating = random.uniform(rating_scale[0], rating_scale[1])\n",
    "    predictions.append((userId, itemId, rating, random_rating, None))\n",
    "\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqp0_9BFq2w8"
   },
   "source": [
    "# Recomendación multimodal para un usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naFpdN49q2w8"
   },
   "source": [
    "Setup del metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiMiS3dwq2w8",
    "outputId": "09145191-786f-44f2-91d8-a89e7d215a42"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0TYGEdLq2w8"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = train_set\n",
    "cols_with_id = {col: idx for idx, col in enumerate(mechanics_df.columns[1:])}\n",
    "\n",
    "# Conjunto de features\n",
    "item_styles = {}\n",
    "\n",
    "for _, row in train_set.iterrows():\n",
    "    bgg_id = row['BGGId']\n",
    "    style_row = mechanics_df[mechanics_df['BGGId'] == bgg_id].drop(columns=['BGGId'])\n",
    "    styles = style_row.columns[style_row.iloc[0] == 1].tolist()\n",
    "    item_styles[bgg_id] = styles\n",
    "\n",
    "print(item_styles)\n",
    "\n",
    "\n",
    "\n",
    "itemslist = df['BGGId'].unique()\n",
    "userslist = df['Username'].unique()\n",
    "stylelist = [i for i in range(len(cols_with_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conjunto de features, pero numericos\n",
    "\n",
    "item_styles_with_ids = {}\n",
    "for item_id, styles in item_styles.items():\n",
    "    style_ids = [cols_with_id[style] for style in styles]\n",
    "    item_styles_with_ids[item_id] = style_ids\n",
    "print(item_styles_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = [(row['Username'], row['BGGId'], row['Rating']) for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "all_features = set(feature for features in item_styles.values() for feature in features)\n",
    "\n",
    "dataset.fit(users=userslist, items=itemslist, item_features=all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(interactions_matrix, weights_matrix) = dataset.build_interactions(\n",
    "    ((x[0], x[1], x[2]) for x in interactions)\n",
    ")\n",
    "\n",
    "item_features = dataset.build_item_features(\n",
    "    ((item_id, features) for item_id, features in item_styles.items())\n",
    ")\n",
    "print(interactions_matrix)\n",
    "# print(item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(no_components=30, loss='warp')\n",
    "model.fit(interactions_matrix, item_features=item_features, epochs=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(model, dataset, user_ids, n_items=5):\n",
    "    n_users, n_items_total = interactions_matrix.shape\n",
    "    item_ids = np.arange(n_items_total)\n",
    "    recommendations_per_user = {}\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        scores = model.predict(user_id, item_ids, item_features=item_features)\n",
    "        top_items = item_ids[np.argsort(-scores)][:n_items]\n",
    "        item_mapping = dataset.mapping()[2]\n",
    "        item_id_mapping = {v: k for k, v in item_mapping.items()}\n",
    "        recommended_items = [item_id_mapping[item] for item in top_items]\n",
    "        print(f\"User {user_id} recommended items: {recommended_items}\")\n",
    "        recommendations_per_user[user_id] = recommended_items\n",
    "    \n",
    "    return recommendations_per_user\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision = precision_at_k(model, interactions_matrix, item_features=item_features, k=5).mean()\n",
    "train_recall = recall_at_k(model, interactions_matrix, item_features=item_features, k=5).mean()\n",
    "\n",
    "print(f'Train precision at k: {train_precision}')\n",
    "print(f'Train Recall: {train_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_group = recommend(model, dataset, [1, 2, 3], n_items=1000)\n",
    "print(type(userslist))\n",
    "print(recommendations_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4vgMrNHWxWl"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ponderación para grupos de usuarios con recomendaciones de metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_df = pd.read_csv('synthetic_groups.csv')\n",
    "groups_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código adaptado de repositorio mencionado anteriormente, en este caso tomamos los items que en promedio son más preferidos para recomendarlos al grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_group(model, dataset, group_members, n_items=5):\n",
    "    n_users, n_items_total = interactions_matrix.shape\n",
    "    item_ids = np.arange(n_items_total)\n",
    "    user_mapping = dataset.mapping()[0]\n",
    "    \n",
    "    group_scores = []\n",
    "    for user_id in group_members:\n",
    "        try:\n",
    "            internal_user_id = user_mapping[user_id]\n",
    "            scores = model.predict(internal_user_id, item_ids, item_features=item_features)\n",
    "            group_scores.append(scores)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if not group_scores:\n",
    "        return []\n",
    "    \n",
    "    average_scores = np.mean(group_scores, axis=0)\n",
    "    top_items = item_ids[np.argsort(-average_scores)][:n_items]\n",
    "    \n",
    "    item_mapping = dataset.mapping()[2]\n",
    "    item_id_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    recommended_items = [item_id_mapping[item] for item in top_items]\n",
    "    \n",
    "    return recommended_items\n",
    "\n",
    "group_recommendations = []\n",
    "for _, row in groups_df.iterrows():\n",
    "    group_members = eval(row['group_members']) if isinstance(row['group_members'], str) else row['group_members']\n",
    "    recommendations = recommend_for_group(model, dataset, group_members, n_items=10)\n",
    "    group_recommendations.append({\n",
    "        'group_members': group_members,\n",
    "        'recommendations': recommendations\n",
    "    })\n",
    "\n",
    "recommendations_df = pd.DataFrame(group_recommendations)\n",
    "recommendations_df.to_csv('group_recommendations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_group_multiple_models(model_name, model, group_members, n_items=10, include_rated=False):\n",
    "    \"\"\"\n",
    "    Generate recommendations for a group using different pyreclab models\n",
    "    Following exact format of original recommend_for_group function\n",
    "    \"\"\"\n",
    "    group_scores = []\n",
    "    \n",
    "    \n",
    "    for user_id in group_members:\n",
    "        print(f\"Recomendacion para user {user_id}\")\n",
    "        try:\n",
    "            ranking = model.recommend(str(user_id), n_items, include_rated)\n",
    "      \n",
    "            recommended_items = ranking\n",
    "            group_scores.append(recommended_items)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting recommendations for user {user_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not group_scores:\n",
    "        return []\n",
    "    \n",
    "    item_counts = {}\n",
    "    for user_recs in group_scores:\n",
    "        for item in user_recs:\n",
    "            item_counts[item] = item_counts.get(item, 0) + 1\n",
    "    \n",
    "    sorted_items = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    recommendations = [int(item) for item, _ in sorted_items[:n_items]]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "models = {\n",
    "    'SVD': mySVD,\n",
    "    # 'UserKNN': myUserKnn,\n",
    "    # 'ItemKNN': myItemKnn,\n",
    "    'MostPopular': myMP\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nGenerating recommendations using {model_name}...\")\n",
    "    group_recommendations = []\n",
    "    \n",
    "    for _, row in groups_df.head(1000).iterrows():\n",
    "        group_members = ast.literal_eval(row['group_members']) if isinstance(row['group_members'], str) else row['group_members']\n",
    "        recommendations = recommend_for_group_multiple_models(model_name, model, group_members, n_items=10)\n",
    "        \n",
    "        group_recommendations.append({\n",
    "            'group_members': str(group_members),\n",
    "            'recommendations': str(recommendations)\n",
    "        })\n",
    "    \n",
    "    recommendations_df = pd.DataFrame(group_recommendations)\n",
    "    recommendations_df.to_csv(f'{model_name.lower()}_group_recommendations.csv', index=False)\n",
    "    print(f\"Generated recommendations for {len(recommendations_df)} groups\")\n",
    "    print(\"Sample format:\")\n",
    "    print(recommendations_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_group_data(groups_df):\n",
    "    parsed_data = []\n",
    "    \n",
    "    for _, row in groups_df.head(30).iterrows():\n",
    "        group_members = ast.literal_eval(row['group_members'])\n",
    "        recommendations = ast.literal_eval(row['recommendations'])\n",
    "        parsed_data.append((group_members, recommendations))\n",
    "    \n",
    "    print(f\"\\nTotal groups parsed: {len(parsed_data)}\")\n",
    "    return parsed_data\n",
    "\n",
    "def calculate_relevance(train_df, user_id, item_id):\n",
    "    rating = train_df[\n",
    "        (train_df['Username'] == user_id) & \n",
    "        (train_df['BGGId'] == item_id)\n",
    "    ]['Rating']\n",
    "    \n",
    "    return rating.iloc[0] if not rating.empty else 0\n",
    "\n",
    "def calculate_group_metrics(parsed_data, train_df, k=5, threshold=3.5):\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_ndcg = 0\n",
    "    total_relevance = 0\n",
    "    total_groups = len(parsed_data)\n",
    "    total_hits = 0\n",
    "    total_possible_hits = 0\n",
    "    all_group_sizes = []\n",
    "    \n",
    "    for i, (group_members, recommendations) in enumerate(parsed_data):\n",
    "        print(f\"\\rProcessing Group {i+1}/{total_groups}\", end=\"\")\n",
    "        recommended_items = recommendations[:k]\n",
    "        group_relevance = []\n",
    "        group_hits = 0\n",
    "        for item_id in recommended_items:\n",
    "            member_ratings = []\n",
    "            for user_id in group_members:\n",
    "                rating = calculate_relevance(train_df, user_id, item_id)\n",
    "                if rating > 0:\n",
    "                    member_ratings.append(rating)\n",
    "                    group_hits += 1\n",
    "                total_possible_hits += 1\n",
    "            avg_rating = np.mean(member_ratings) if member_ratings else 0\n",
    "            group_relevance.append(avg_rating)\n",
    "        \n",
    "        total_hits += group_hits\n",
    "        \n",
    "        binary_relevance = [1 if rel >= threshold else 0 for rel in group_relevance]\n",
    "        \n",
    "        total_relevant = 0\n",
    "        unique_items = train_df['BGGId'].unique()\n",
    "        for item_id in unique_items:\n",
    "            member_ratings = []\n",
    "            for user_id in group_members:\n",
    "                rating = calculate_relevance(train_df, user_id, item_id)\n",
    "                if rating > 0:\n",
    "                    member_ratings.append(rating)\n",
    "            avg_rating = np.mean(member_ratings) if member_ratings else 0\n",
    "            if avg_rating >= threshold:\n",
    "                total_relevant += 1\n",
    "        \n",
    "        precision = sum(binary_relevance) / k if k > 0 else 0\n",
    "        recall = sum(binary_relevance) / total_relevant if total_relevant > 0 else 0\n",
    "        \n",
    "        dcg = sum((2 ** rel - 1) / np.log2(idx + 2) \n",
    "                  for idx, rel in enumerate(group_relevance))\n",
    "        ideal_relevance = sorted(group_relevance, reverse=True)\n",
    "        idcg = sum((2 ** rel - 1) / np.log2(idx + 2) \n",
    "                   for idx, rel in enumerate(ideal_relevance))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        \n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_ndcg += ndcg\n",
    "        total_relevance += np.mean(group_relevance) if group_relevance else 0\n",
    "    \n",
    "    avg_metrics = {\n",
    "        \"average_precision\": total_precision / total_groups,\n",
    "        \"average_recall\": total_recall / total_groups,\n",
    "        \"average_ndcg\": total_ndcg / total_groups,\n",
    "        \"average_relevance\": total_relevance / total_groups,\n",
    "        # \"hit_rate\": total_hits / total_possible_hits,\n",
    "        # \"average_group_size\": np.mean(all_group_sizes),\n",
    "        # \"total_groups\": total_groups,\n",
    "        # \"total_hits\": total_hits,\n",
    "        # \"total_possible_hits\": total_possible_hits\n",
    "    }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "groups_df = pd.read_csv('group_recommendations.csv')\n",
    "parsed_data = parse_group_data(groups_df)\n",
    "metrics = calculate_group_metrics(parsed_data, train_df, k=5)\n",
    "print(f\"Precision: {metrics['average_precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['average_recall']:.3f}\")\n",
    "print(f\"nDCG: {metrics['average_ndcg']:.3f}\")\n",
    "print(f\"Relevance Score: {metrics['average_relevance']:.2f}\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "groups_df = pd.read_csv('svd_group_recommendations.csv')\n",
    "parsed_data = parse_group_data(groups_df)\n",
    "metrics = calculate_group_metrics(parsed_data, train_df, k=5)\n",
    "print(f\"Precision: {metrics['average_precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['average_recall']:.3f}\")\n",
    "print(f\"nDCG: {metrics['average_ndcg']:.3f}\")\n",
    "print(f\"Relevance Score: {metrics['average_relevance']:.2f}\")\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "groups_df = pd.read_csv('mostpopular_group_recommendations.csv')\n",
    "parsed_data = parse_group_data(groups_df)\n",
    "metrics = calculate_group_metrics(parsed_data, train_df, k=5)\n",
    "print(f\"Precision: {metrics['average_precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['average_recall']:.3f}\")\n",
    "print(f\"nDCG: {metrics['average_ndcg']:.3f}\")\n",
    "print(f\"Relevance Score: {metrics['average_relevance']:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasta aca esta probado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell for AGREE implementation\n",
    "# New cell for AGREE implementation\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from scipy import sparse\n",
    "\n",
    "# Model classes\n",
    "class UserEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim):\n",
    "        super(UserEmbeddingLayer, self).__init__()\n",
    "        self.userEmbedding = nn.Embedding(num_users, embedding_dim)\n",
    "    \n",
    "    def forward(self, user_inputs):\n",
    "        return self.userEmbedding(user_inputs)\n",
    "\n",
    "class ItemEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim):\n",
    "        super(ItemEmbeddingLayer, self).__init__()\n",
    "        self.itemEmbedding = nn.Embedding(num_items, embedding_dim)\n",
    "    \n",
    "    def forward(self, item_inputs):\n",
    "        return self.itemEmbedding(item_inputs)\n",
    "\n",
    "class GroupEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_groups, embedding_dim):\n",
    "        super(GroupEmbeddingLayer, self).__init__()\n",
    "        self.groupEmbedding = nn.Embedding(num_groups, embedding_dim)\n",
    "    \n",
    "    def forward(self, group_inputs):\n",
    "        return self.groupEmbedding(group_inputs)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0.1):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_ratio),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        weight = torch.softmax(out.view(1, -1), dim=1)\n",
    "        return weight\n",
    "\n",
    "class PredictLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, drop_ratio=0.1):\n",
    "        super(PredictLayer, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_ratio),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class AGREE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_groups, embedding_dim, group_member_dict, drop_ratio=0.1):\n",
    "        super(AGREE, self).__init__()\n",
    "        self.userembeds = UserEmbeddingLayer(num_users, embedding_dim)\n",
    "        self.itemembeds = ItemEmbeddingLayer(num_items, embedding_dim)\n",
    "        self.groupembeds = GroupEmbeddingLayer(num_groups, embedding_dim)\n",
    "        self.attention = AttentionLayer(2 * embedding_dim, drop_ratio)\n",
    "        self.predictlayer = PredictLayer(3 * embedding_dim, drop_ratio)\n",
    "        self.group_member_dict = group_member_dict\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight)\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "    \n",
    "    def forward(self, group_inputs, user_inputs, item_inputs):\n",
    "        if (group_inputs is not None) and (user_inputs is None):\n",
    "            return self.group_forward(group_inputs, item_inputs)\n",
    "        else:\n",
    "            return self.user_forward(user_inputs, item_inputs)\n",
    "    \n",
    "    def group_forward(self, group_inputs, item_inputs):\n",
    "        group_embeds = torch.Tensor()\n",
    "        item_embeds_full = self.itemembeds(torch.LongTensor(item_inputs))\n",
    "        \n",
    "        for i, j in zip(group_inputs, item_inputs):\n",
    "            members = self.group_member_dict[i.item()]\n",
    "            members_embeds = self.userembeds(torch.LongTensor(members))\n",
    "            items_numb = [j] * len(members)\n",
    "            item_embeds = self.itemembeds(torch.LongTensor(items_numb))\n",
    "            \n",
    "            group_item_embeds = torch.cat((members_embeds, item_embeds), dim=1)\n",
    "            at_wt = self.attention(group_item_embeds)\n",
    "            g_embeds_with_attention = torch.matmul(at_wt, members_embeds)\n",
    "            group_embeds_pure = self.groupembeds(torch.LongTensor([i]))\n",
    "            g_embeds = g_embeds_with_attention + group_embeds_pure\n",
    "            group_embeds = torch.cat((group_embeds, g_embeds))\n",
    "        \n",
    "        element_embeds = torch.mul(group_embeds, item_embeds_full)\n",
    "        new_embeds = torch.cat((element_embeds, group_embeds, item_embeds_full), dim=1)\n",
    "        return torch.sigmoid(self.predictlayer(new_embeds))\n",
    "    \n",
    "    def user_forward(self, user_inputs, item_inputs):\n",
    "        user_embeds = self.userembeds(user_inputs)\n",
    "        item_embeds = self.itemembeds(item_inputs)\n",
    "        element_embeds = torch.mul(user_embeds, item_embeds)\n",
    "        new_embeds = torch.cat((element_embeds, user_embeds, item_embeds), dim=1)\n",
    "        return torch.sigmoid(self.predictlayer(new_embeds))\n",
    "\n",
    "# Load and prepare data\n",
    "groups_df = pd.read_csv('synthetic_groups.csv').iloc[:10000]\n",
    "groups_df['group_members'] = groups_df['group_members'].apply(literal_eval)\n",
    "\n",
    "# Convert sparse matrix to dense or CSR format for indexing\n",
    "if sparse.issparse(interactions_matrix):\n",
    "    interactions_matrix = interactions_matrix.tocsr()\n",
    "\n",
    "# Create mappings and initialize model\n",
    "all_users = set()\n",
    "for members in groups_df['group_members']:\n",
    "    all_users.update(members)\n",
    "\n",
    "user_to_idx = {old_id: idx for idx, old_id in enumerate(sorted(all_users))}\n",
    "item_to_idx = {old_id: idx for idx, old_id in enumerate(range(interactions_matrix.shape[1]))}\n",
    "group_member_dict = {\n",
    "    group_id: [user_to_idx[user] for user in members]\n",
    "    for group_id, members in enumerate(groups_df['group_members'])\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 32\n",
    "model = AGREE(\n",
    "    num_users=len(user_to_idx),\n",
    "    num_items=len(item_to_idx),\n",
    "    num_groups=len(groups_df),\n",
    "    embedding_dim=embedding_dim,\n",
    "    group_member_dict=group_member_dict\n",
    ")\n",
    "\n",
    "# Train model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(5):  # 5 epochs as example\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Train on individual interactions\n",
    "    for user_idx in range(interactions_matrix.shape[0]):\n",
    "        # Get non-zero elements for this user\n",
    "        items = interactions_matrix[user_idx].nonzero()[1]\n",
    "        \n",
    "        for item_idx in items:\n",
    "            user_tensor = torch.LongTensor([user_idx])\n",
    "            item_tensor = torch.LongTensor([item_idx])\n",
    "            \n",
    "            prediction = model(None, user_tensor, item_tensor)\n",
    "            target = torch.FloatTensor([[1.0]])  # Changed to 2D tensor\n",
    "            \n",
    "            loss = criterion(prediction, target)\n",
    "            \n",
    "            # Negative sampling\n",
    "            neg_item_idx = np.random.randint(0, interactions_matrix.shape[1])\n",
    "            while interactions_matrix[user_idx, neg_item_idx] != 0:\n",
    "                neg_item_idx = np.random.randint(0, interactions_matrix.shape[1])\n",
    "            \n",
    "            neg_item_tensor = torch.LongTensor([neg_item_idx])\n",
    "            neg_prediction = model(None, user_tensor, neg_item_tensor)\n",
    "            neg_target = torch.FloatTensor([[0.0]])  # Changed to 2D tensor\n",
    "            \n",
    "            loss += criterion(neg_prediction, neg_target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/5, Loss: {total_loss}')\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Function to get recommendations for a group\n",
    "def get_group_recommendations(model, group_idx, num_items, top_k=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = []\n",
    "        group_tensor = torch.LongTensor([group_idx])\n",
    "        \n",
    "        for item_idx in range(num_items):\n",
    "            item_tensor = torch.LongTensor([item_idx])\n",
    "            score = model(group_tensor, None, item_tensor)\n",
    "            scores.append((item_idx, score.item()))\n",
    "        \n",
    "        # Get top-k recommendations\n",
    "        recommendations = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        return [item_idx for item_idx, _ in recommendations]\n",
    "\n",
    "# Get recommendations for first group\n",
    "group_idx = 0\n",
    "recommendations = get_group_recommendations(model, group_idx, interactions_matrix.shape[1])\n",
    "print(f\"\\nTop 10 recommendations for group {group_idx}:\")\n",
    "print(recommendations)\n",
    "\n",
    "# Optional: Get recommendations for all groups\n",
    "print(\"\\nGenerating recommendations for all groups...\")\n",
    "all_group_recommendations = {}\n",
    "for group_idx in range(len(groups_df)):\n",
    "    recommendations = get_group_recommendations(model, group_idx, interactions_matrix.shape[1])\n",
    "    all_group_recommendations[group_idx] = recommendations\n",
    "    print(f\"Group {group_idx}: {recommendations}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
